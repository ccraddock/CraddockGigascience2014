\section*{Introduction}

With its new emphasis on collecting larger datasets, data sharing, deep phenotyping, and multimodal integration, nueroimaging has become a data intensive science. This is particularly true for connectomics where grass-root initiatives (e.g.the 1000 Functional Connectomes Project (FCP) \cite{Biswal2010}, the International Neuroimaging Data-sharing Initiative (INDI) \cite{Mennes2013}) and large-scale interational projects (the Human Connectome Project\cite{Sotiropoulus2013,VanEssen2012}, the Brain Connectome project from China\cite{Jiang2013}, the human brain project in EU known as CONNECT\cite{Assaf2013}, PING, NDAR) are collecting and openly sharing thousands of brain imaging scans, each of which consist of hundreds of observations of thousands of variables. Although this deluge of complex data promises to enable the investigation of nueroscientific questions that were previously inaccessible, it is quickly overwhelming the capacity of existing tools and algorithms to extract meaningful information from the data. This combined with a new focus on discovery science is creating a plethora of opportunities for data scientists from a wide range of disciplines such as computer science, engineering, mathematics, statistics, etc., to make substantial contributions to neuroscience. The goal of this review is to describe the state-of-the-art in connectomics research and enumerate opportunities for data scientists to contribute to the field.

The human connectome is a comprehensive map of the brain's circuitry, which consists of brain areas, their structural connections and their functional interactions. The connectome can be measured with a variety of different imaging techniques, but magnetic resonance imaging (MRI) is the most common primarily due to its near-ubiquity, non-invasiveness, and high spatial resolution. As measured by MRI: brain areas are patches of cortex (approximately 1\si{\centi\meter\cubed} volume) containing 1,000s of neurons, structural connections are long range fiber tracts that are inferred from the motion of water particles measured by diffusion weighted MRI (dMRI), and functional interactions are inferred from synchronized brain activity measured by functional MRI (fMRI). Addressing the current state-of-the-art for both functional and structural connectivity is well beyond the scope of a single review. Instead, this review will focus on functional connectivity, which is particularly fast growing and offers many exciting opportunities for data scientists.

The advent of functional connectivity analyses has popularized the application of discovery science to brain function, which marks a shift in emphasis from hypothesis testing, to supervised and unsupervised methods for learning statistical relationships from the data. Since functional connectivity is inferred from statistical dependencies between physiological measures of brain activity (i.e. correlations between the dependent variables), it can be measured without an experimental manipulation. Thus, functional connectivity is most commonly measured from "resting state" fMRI scans, during which the study participant is lying quietly and not performing an experimenter specified task - when measured in this way, it is referred to as intrinsic functional connectivity (iFC). Once iFC is measured, data mining techniques can be applied to identify iFC patterns that covary with phenotypes, such as, indices of cognitive abilities, personality traits, or disease state, severity, and prognosis, to name a few. In a time dominated by skepticism about the ecological validity of psychiatric diagnoses, iFC analyses have become particularly important for identifying subgroups within patient populations by similarity in brain architecture, rather than similarity in symptom profiles. This new emphasis in discovery necessitates a new breed of data analysis tools that are equipped to deal with the issues inherent to functional nueroimaging data.

No matter what type of method is being applied, iFC analyses are plagued by the curse of dimensionality and an inability to validate their results with a gold standard. Functional MRI datasets typically involve hundreds or thousands of scans, each of which consist of a time series of hundreds of 3D brain volumes, that each contain measures from hundreds of thousands of brain locations (variables). Whether performing an analysis within a scan, or across scans, the number of variables (N) is much larger than the number of observations (P). Different analyses have approached the dimensionality problems in different ways, but there is no consensus on the best algorithm for brain data. Indeed there isn't a consensus about the best methods to use for any step of iFC analyses, and this lack of agreement is due to a lack of a ground truth or gold standard for comparing different techniques. Although several simulations have been proposed, there is always some skepticism about their accuracy and comprehensiveness. In their place, cross validation techniques have been utilized to compare methods based on some measure of reproducibility and/or generalizability.

This review is a primer on intrinsic functional connectivity analyses for interdisciplinary data scientists. It begins in section 2 by providing a description of fMRI data and the technical issues encountered when working with the data. Section 3 highlights several cutting edge iFC analytical paradigms that are being employed, and open issues that will benefit from an increased engagement of data science practitioners. The review concludes in section 4 with a list of several valuable open science resources that make connectomics research accessible to the larger scientific community.

\section{The Data}

Functional MRI, in its most conventional form, is a brain imaging modality in which approximately 40 3-\si{\milli\meter}, $64 \times 64$ voxel (3D pixel, $3\times3$\si{\milli\meter\squared} in-plane resolution), brain slices are sequentially acquired every 2 - 3 seconds. The fMRI signal relies on the blood oxygenation level dependent (BOLD) contrast to derive a relative measure of brain function. The BOLD signal originates in the magnetic properties of hemoglobin, the protein in blood that binds to oxygen. When hemoglobin is not bound to oxygen (deoxygenated), its four atoms of iron generate a local magnetic field gradient that dephases a region of the fMRI signal. When bound to oxygen (oxygenated) the influence of the iron on the magnetic field is blocked, and the fMRI signal is preserved. The magnitude of the fMRI signal in an image voxel is proportional to the ratio between oxygenated and deoxygenated hemoglobin in or near that voxel. Perhaps quixotically, when brain activity increases, so does the oxy-deoxy ratio, resulting in a increase in the magnitude of the fMRI signal \cite{fmribook}. 

The changes in BOLD due to brain activity (hemodynamic response), as measured using task-fMRI, are fairly slow, peaking 1-2 seconds after neuronal activity begins, and persisting 4-6 seconds after it ends \cite{fmribook}. Resting state fluctuations that underpin iFC are consistently localized to the between $0.01-0.08$ \si{\hertz}, with a peak somewhere around $0.035$ \si{\hertz}\cite{biswal,cordes}. Since the BOLD signal originates from magnetic fields generated by blood, it tends to be localized to the larger vascular and smears out beyond an active brain region. As a consequence of these two properties of BOLD, there is temporal and spatial auto-correlation in fMRI data that must be accounted for in the analysis (e.g. when estimating statistical significance).

Although fMRI data contains several sources of noise, such as linear drifts and Rician distributed noise from the MRI hardware\cite{Gudbjartsson1995}, it is dominated by physiological noise. The scanning participant's heart beat generates a pulsatile motion in the brain and consequently fluctuations in the fMRI signal\cite{birn2012}. Changes in abdominal volume due to breathing causes variations in the magnetic field profile, which in turn, result in global fluctuations in image intensity\cite{birn2012}. Also, changes in rate and depth of respiration will lead to changes in the brain's oxygenation level and longer-term modulations of the fMRI signal\cite{birn2012}. Perhaps worse of all is participant head motion, which not only results in the misalignment of brain regions between brain volumes, but also modulates fMRI signal intensity due to partial volumning and spin-history effects. Although head motion can be mitigated during the acquisition process, other sources of noise can only be removed using post-hoc corrections to the fMRI signal.

\section*{Data preprocessing}

Several preprocessing procedures have been developed to precondition fMRI data to remove noise and other sources of systematic variation that might confound analysis results. fMRI data is acquired sequentially one slice at a time, resulting in phase delays that vary between slices, these delays can be removed by interpolating all of the data to the same time grid. Imaging volumes can be spatially co-registered to correct their misalignment due to head motion. Although low frequency scanner drift can be removed from the data by high-pass filtering ($f<0.01$  \si{\hertz}), the slow sampling rates of typical fMRI sequences ($0.33 \leq f_s \leq 0.5$ \si{\hertz}) make it impossible to filter out the higher frequency ($1 \leq f \leq 2$ \si{\hertz}) contributions of physiological noise. Instead heart rate, respiration, and head motion effects are modeled using a regression framework, and the result is subtracted from the fMRI data \cite{lund_nvr}. For heart beat and respiration this can be accomplished using physiological recordings of these signals \cite{Hu1999,Glover2002}, but due to the difficulties of making these recordings in the scanning environment, surrogate signals from cerebrospinal fluid and white matter, which contain fluctuations due to physiological noise but no neuronal signal, are commonly used to model their effects. Several different methods have been proposed to account for head-motion induced fluctuations in the fMRI signal, and most include either the 6 motion parameters ($\delta_{x}[t]$, $\delta_{y}[t]$, $\delta_{z}[t]$, pitch, roll, yaw) calculated during coregistration\cite{fox2005}, or a 24 parameter model derived from this parameters (the six parameters, their squares, the 1-lagged parameters, and the square of the 1-lag parameters)\cite{friston1996}, with some evidence supporting the 24-parameter model as best\cite{Yan2013, Satterthwaite2013}. More aggressive methods censor offending time points by either deleting them all-together\cite{Power2012} or in a regression model (spike regression)\cite{gabrieli-whitefield}. Some researchers have advocated regressing out the global-signal as a non-specific measure of noise\cite{fox_response}, but this method is widely criticized since it mathematically centers the distribution of iFC values, which jeopardizes the interpretation of negative correlations\cite{kevin,saad}.  

Preprocessing fMRI data is reliant on the analysis of high resolution structural MRI data that is usually acquired during the same scanning session. It is from the structural data that we can segment the images into cerebral spinal fluid, white matter, and gray matter areas of the brain. Additionally the structural data is used in to calculate a spatial transformation to a brain template to account for volumetric and morphometric differences between participants. In general, the quality of iFC analyses depends on the quality of the structural data and its processing.

\subsection*{IFC and Regional measures}

A variety of different strategies have been developed for analyzing iFC, that are outside what might be considered \emph{connectomes}-style analyses, but nevertheless are valuable methods for investing the brain's functional architecture. iFC began with seed-based correlation analyses, which identify an iFC-map for a particular seed region from the Pearson's correlation of that regions mean time course with the time courses of every other voxel in the brain \cite{biswal1995}. This hypothesis-oriented approach has been complemented by unsupervised approaches such as clustering and independent components analysis for defining \emph{intrinsic connectivity networks} (ICN). Indeed using ICA researchers have identified 6-10 ICNs which are reproducible across participants, time, and across cognitive tasks. Several regional measures have been proposed that provide sensitivity to different aspects of the data's structure. The fractional amplitude of of low frequency fluctuations is a normalized measure of each voxel's power in the frequency band commonly associated with iFC (0.01-0.08 Hz) and provides a relative measure of brain activity. Regional homogeneity (ReHo) measures the degree to which a voxel's 6-, 18-, or 22- voxel neighborhood are synchronized. Voxel-mirrored homotopic connectivity is measured from the correlation between a voxel's time course and the time course for its corresponding voxel in the contralateral hemisphere. Another approach, which is measured from the time-lag that maximizes correlation between a voxel's time course and a reference time course such as the global signal, has been shown to be sensitive to blood flow deficits, particularly in stroke \cite{lv}. The result of each of these methods is a voxel-wise map of statistics that can be compared across participants using either voxel-by-voxel univariate statistical tests such as t-tests or anovas, or can be analyzed using multivariate supervised learning techniques.\todo{Add in lfcd}

\section{The Connectome Analysis Paradigm}

In 2005 Sporn and Hagmann \cite{Sporns2005,Hagmann2005} independently and in parallel coined the term \textit{the human connectome}, which embodies this notion that the set of all connections within the human brain can be represented and understood as graphs. In the context of iFC graphs provide a mathematical representation of the functional interactions between brain areas. In this representation \emph{nodes} are brain areas that are connected by \emph{edges} that indicate their functional connectivity. Any graph analysis problem follows the following procedural steps: (1) identification of nodes, (2) identification of edges i.e. the associations between nodes, (3) analysis of the graph i.e. the structure and edges of the resulting graph in order to identify any expected and novel patterns, and (4) validation of the resulting patterns. Analysis of the human connectome adheres to these steps as outlined in this section. 

\subsection*{Defining the nodes of the connnectome}

The first step in constructing brain graphs from fMRI data is the specification of nodes. One approach might be to use voxels as nodes, but that will result in computationally expensive graphs that can contain billions of edges. Due to the spatial correlation inherent in the data, neighboring voxels are sufficiently similar to be combined together into larger brain areas. It is important for nodes to be functional homogenous, as any mixing of time courses between different functional areas will result in an inaccurate reproduction of brain connectivity \cite{smith2010}. Alternatively, nodes can be circumscribed using anatomical atlases, meta-analyses of functional studies, or data-driven parcellation approaches. Anatomical atlases subdivide the brain based on anatomical landmarks\cite{AAL, HO}, such as sulcal patterns\cite{varoquaux}, or into areas containing similar cell types, as determined by microscopic visualization of post-mortem brains\cite{TT,Eickhoff-zilles}. But, since the definition of the brain areas in these atlases did not incorporate functional information, they are not likely to be functionally homogenous. Another approach is to define regions from the overlap of activations found in other functional nueroimaging studies\cite{Dosenbach}, which works well as long as the regions are relatively similar in size and are non-overlapping\cite{VaroquauxCraddock2013}. In the last approach, a unsupervised or semi-supervised learning algorithm can be employed to subdivide functional data into homogenous units. Several different algorithms have been applied to data-driven node specification that vary in definition of clustering cost, incorporation of explicit constraints, and whether they are performed at the individual or group-level. This prompts the inevitable question of which parcellation is the best representation of the human connectome. Overall there is no consensus on which is the best node definition to use, although there is strong evidence that data-driven approaches outperform anatomical atlases (figure from cameron paper).

A considerable drawback of data-driven approaches for defining brain regions, is that the number of regions in the clustering solution, or conversely their size, must be specified. The choice of node size has been shown to have a considerable effect on the resulting graph topology \cite{Zalesky} and can also affect the outcome of a particular study\cite{Cecci_2009}. There are a variety of mechanisms for optimizing the number of brain areas in a parcellation, many of which using cross-validation methods, but in application they do not tend to converge to a single optimum, and instead provide a range of parcellations that are suitable\cite{craddock2012}. The choice of parcellation used will strictly depend on the problem to be solved by that analysis and the amount of error that one can allow when interpreting the results. 

\subsection*{Define the edges}
After deciding the grain size of each region, i.e. whether it be an individual voxel or a set of spatially contiguous voxels or even a set of overlapping regions (one voxel can belong to more than one node of the graph e.g. ICA approach), one needs to construct the edges that describe relationships between nodes. While general graphs can have multiple edges between two nodes, brain graphs tend to be simple graphs with a single undirected edge between pairs of nodes (i.e. the direction of influence between nodes is unknown). Additionally edges in graphs of brain function tend to be weighted - annotated with a value that indicates the similarity between nodes. 

The first step in defining edges is summarizing the functional information present in the node. A representative time course for each node may be created by either averaging the time courses of all the voxels in a node, or by taking the first eigenvariate from a single value decomposition of all of the time courses\cite{friston_functional_localizers}. The former is preferred because it has a tendency to represent all of the time courses in a node equally, rather than favoring those that are most commonly occurring \cite{craddock2012}. These summary time courses can either be compared directly between nodes, or can be used in a seed-based correlation analysis to construct node-specific whole brain iFC maps, which are then compared spatially between nodes. Although, these two features are seemingly similar, they can lead to very different similarities between nodes \cite{craddock2012}. 

Quite a few different measures have been used in the iFC literature for measuring the similarity between time courses, many of which have been reviewed in an extensive simulation study\cite{smith2010}. By far the most commonly employed measure is Pearson's correlation coefficient, which performs well in simulation\cite{smith2010}. A benefit of Pearson's correlation is that is scaled so that it is insensitive to differences in scales and shifts between time courses. This is particularly useful for fMRI, which is a relative measure of brain function that can vary widely across the brain for reasons that are non-neuronal in source, i.e. proximity to a vein. A draw back is that Pearson's correlation is a bivariate measure that does not adequately address the simultaneous influence that multiple nodes have on one another. The resulting loss of specificity might result in "phantom" edges between regions that is a by-product of shared variance from a mutual relationship with another region (or several other regions). Partial correlation methods simultaneously take into account all of the nodes in their calculation and as such have higher specificity. But, since there are often more nodes in a graph than independent observations (particularly when accounting for temporal autocorrelation) partial correlations cannot be directly measured from the data. Instead, regularized regression techniques can be employed\cite{VaroquauxCraddock2013}. Approaches for measuring the spatial similarity between iFC maps has been much less studied in the literature. Commonly employed measures include $\eta^2$\cite{} and the concordance correlation coefficient\cite{}, both of which measure how identical the two maps are, included differences in shifts and scales. We have yet to find a report where partial correlation was used to calculate similarity between iFC maps.

Once the similarity between nodes has been calculated, a threshold must be chosen to use to indicate the presence of an edge. Negative relationships are possible given the commonly employed similarity measures, but these are typically removed from the graph since it is unclear how to interpret negative similarity, and because many graph theoretical mathematical techniques require the weights to be nonnegative. It is common to apply a significance threshold to the graph, which is chosen to minimize the number of edges that arise by chance. These thresholds can be determined by parametric tests such as Pearson's correlation to p-value calculators, or non-parametric tests such as wave-strapping\cite{breakspear}, circular block bootstrap\cite{bellec}, or phase randomization\cite{zhen}. It is import to correctly account for the auto-correlation inherent in the data when employing these techniques. Alternatively, a sparsity threshold may be employed to choose a percentage of the edges with the strongest weights. This is useful when calculating graph statistics that are highly sensitive to the number of edges in the graph, and may serve to ameliorate batch-effects that may exist between datasets acquired on different scanners or at different sites\cite{Chaogan}. Once the below-threshold edges have been excluded from the brain graphs, the are ready for group-level analysis. 

\section*{Comparing brain graphs} 

The ultimate goals of connectomics is to map the brains functional architecture and to annotate that architecture with the cognitive or behavioral functions that they subtend. This latter pursuit is achieved by a group level analysis in which variations in the connectome are mapped to inter-individual variations in phenotype\cite{Kelly2011}, or intra-individual responses to experimental perturbations (e.g. \cite{Shirer}). Several different analyses have been proposed for accomplishing these goals, and they all require some mechanism for comparing brain graphs. 
 
\subsection*{How to measure similarity/distance between graphs}
Approaches to comparing brain graphs can be divided into several types based on how they treat the statistical relationships between edges. One such approach, referred to as \emph{bag of edges} is to treat each edge in the brain graph as a sample from some random variable. Thus, a set of $N$ brain graphs each with $M$ edges will have $N$ observations for each of the $M$ random variables. In this case, the adjacency (or similarity) matrix that describes the brain graphs can be flattened into a vector representation and any of the well explored similarity or dissimilarity metrics can be applied to the data \cite{Ravindran}. One of the benefits of this representation is the ability to treat each edge as independent of all other edges and to compare graphs using mass univariate analysis, in which a separate univariate statistical test (e.g. t-test, anova, or ancova) is performed at each edge. This will result in a very large number of comparisons and an appropriate correction for multiple comparisons, such as Network Based Statistic \cite{Zalesky2011}, Spatial Pairwise Clustering \cite{Zalesky2012}, Statistical Parametric Networks \cite{Ginestet2013}, or group-wise false discovery rate \cite{}, must be employed to control the number of false positives. Alternatively the interdependencies between edges can be modeled at the node level using multivariate distance multiple regression (MDMR) \cite{Shehzad2014}, or across all edges using machine learning methods \cite{Craddock2009, Dosenbach2010,Richiardi2011}. Despite the successful application of this technique, a drawback of representing a brain graph as a bag of edges is that this representation throws away all information about the structure of the graph.  In an effort to overcome these limitations, work is being done to look at sub-graphs \cite{} 

Another approach for graph similarity using all the vertexes involves computing a set of \emph{graph-invariants} such as node centrality, modality, global efficiency e.t.c. and using the values of these measures to represent the graph \cite{rubinov} \todo{find more}. The advantages of this approach is that they do not require graphs to be aligned, and can substantially reduce the dimensionality of the graph comparison problem. On the other hand, representing the graph using its computed invariants throws away information about that graph's vertex labels \cite{Vogelstein2012}. Moreover, after computing these invariants it is often unclear how they can be interpreted biologically. \todo{how about directly comparing graphs using a graph similarity metric}

\section*{Future Trends}

\subsection*{Dynamic Connectivity}
One important assumption that is made when building brain networks is that the fMRI signal is stationary. This simplification affords the use of correlation coefficients (computed using the entire representative time-course of each brain region) for representing the statistical dependence between brain regions. Recent studies \cite{Hutchinson2013, Fu2013,} however, have demonstrated that removing this assumption and using techniques that rightfully assume that the fMRI signal is not stationary across time, results in brain networks that show changes in connectivity patterns between brain regions over time. These results mirror what scientists have come to expect; brain regions change allegiances during the course of executing various tasks.

To date trying to model the human connectome via taking into account its dynamic nature posses a number of unsolved problems \cite{Hutchinson2013} some of which include: (1) identifying and removing contributions to the modeled functional connectome due to subject motion, scanner noise, and physiological noise, (2) given that in dynamic connectivity the "brain network" changes with time, what is the best way to define its nodes and edges (ie. quantify the dynamic connectivity), (3) what are the roles of individual brain regions (including subcortical structures and brain stem) in regulating these dynamic networks, and (4) are there specific instances of network configurations that are repeated/revisited over time? For more open problems and a detailed explanation of work done in the area of dynamic functional connectivity see \cite{Hutchinson2013}.

\subsection*{Causality}
To date, a lot of work has been done to identify nodes within a brain graph that share some degree of mutual information (known as functional connectivity). Another interesting question to answer with regards to brain graphs is to identify what nodes have direct influence on the signal obtained in other nodes (known as effective connectivity). In other words, how can we identify the directional causal influence of one brain region over another using brain graphs? This is a hard problem to tackle using rsfMRI. Simple correlation analysis does not suffice because a "high correlation between remote sampling sites might imply some direct connection, or it might imply some third site driving their joint activation, or it might imply them jointly driving some third site. And even if they are connected, it's difficult to tell the direction of the connection, even if there is a "direction" to it." \footnote{http://mindhive.mit.edu/node/58}. 

Current approaches include the use of dynamic causal modeling (DCM) \cite{}, structural equation modeling (SEM) \cite{}, and Granger Causality \cite{}. While these methods have shed some light into the wiring of the brain, there are some unresolved problems. DCM analysis requires some external stimuli that will perturb a given brain networks. Such a stimuli is not possible for rsfMRI data because it is acquired when the subject is at rest. On the other hand, using the GC approach one attempts to identify a specific node that influences the characteristic of another node in a given brain network. The drawback of this approach is that the hemodynamic response in fMRI signal naturally lags by a few seconds. Thus, it is non-trivial to draw conclusions that any observed lag in signal is specifically due to one node exerting an influence over another.   

\subsection*{Prediction}
There is an increase in the use of machine learning techniques especially supervised learning in neuroimaging for predicting various aspects of brain function and/or disfunction. Researchers have been able to train computers to learn patterns that can discriminate between different cohorts e.g. disease individuals versus healthy controls \cite{Richiardi2013}. The learnt patterns are then used to identify the distinguishing characteristics between the cohorts. For some learning algorithms, the resulting characteristics could subsequently be used to validate some hypothesis about brain function and/or malfunction \cite{} e.g. identifying patterns that result into neurodegenerative disorders \cite{WeeMICCAI2013}. 

The three basic steps in supervised learning are: (1) generate the features (independent variables), (2) give each subject a label representing what group they belong to (dependent variable), and (3) apply an appropriate machine learning algorithm to the generated features and labels. The features for the brain graphs can be (1) a set of topological properties from each brain graph \cite{Cecci2009, Bassett2012}, (2) a vector embedding of the brain graphs \cite{Richiadi2013,Luo2003}, or (3) the result of passing the brain graphs through a graph kernel \cite{}. 

As with other predictive problems in data analysis that use graphs as features, there are several open problems that need to be investigated: (1) Feature Representation: what is the appropriate approach for presenting each brain graph to the learning algorithms. Is it one of the above mentioned approaches or could one create new representations that will make the learning algorithms more sensitive to the differences and similarities between brain graphs. (2) Scalability: as the imaging technology is getting better over time, we expect the resolution of fMRI images to also improve enabling the generation of brain graph at finer scales (ie. with more regions). How can we best scale available learning algorithms to accommodate this change? As noted by Richiardi et. al. \cite{Richiardi} a typical learning algorithm to date can only handle graphs with up to a hundred or a few thousand nodes at best. And even then, the available data tends to have much less observations compared to the predictors (brain voxels/regions). This has been shown to cause poor performance of many learning algorithms \cite{Hastie}. A potential solution is to use regularization techniques \cite{WeeMICCAI}. Are there other approaches that could be used? (3) How can we account for the bias introduced during the process of estimating statistical dependence between regions? This bias comes from the number of timepoints available for each time-course (the larger this value the more confident we can be that the resulting correlation coefficient is trully different from zero), significant variations in size of many brain regions, physiological noise, and noise with spatial characteristics \cite{Richiardi}

\section*{Conclusion}

\section*{Open Science Resources}
As with other big data problems, the analysis of the type of brain data described above will benefit significantly from open science and open data. By open science we mean the process of making intermediate and end data products available to researchers outside the lab that initiated the investigation and by open data we mean data sharing, data reuse, allowing groups other than the ones that acquired the data to further analyze freely available data \cite{Milham2012}. Both open science and open data have the benefit of facilitating the creation of new questions to answer from a data set that the people who acquired the data didn't conceive. A great example of this is the bioinformatics community, which came into existence due to the availability of large genomics datasets that were publicly available and could be used by anybody who possessed the skills and knowledge to mine them\cite{VanHorn2013}. This brought scientists and researchers from disparate fields (ranging from biology, statistics, to computer science and engineering) together. The same can be aspired for the brain imaging community by making data acquired in different labs publicly available. At the moment there are a few public databases such as the fMRI data center \footnote{fmri datacenter webaddress}, LONI IDA \footnote{LONI webaddress}, 1000 Functional Connectomes Project (FCP)\footnote{\url{http://fcon_1000.projects.nitrc.org}}, the International Neuroimaging Data-sharing Initiative (INDI)\footnote{INDI webaddress}, ABIDE preprocessed\footnote{\url{http://preprocessed-connectomes-project.github.io/abide/}}, Human Connectome Project \footnote{HCP webaddress}, ADHD 200 preprocessed \footnote{\url{http://neurobureau.projects.nitrc.org/ADHD200/Introduction.html}},CORR \footnote{\url{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/index.html}}. Among these, there are a few such as the data from the Human Connectome Project and the Cameron data set, which contain clean and tidy data. The latter will speed up knowledge discovery and new method development by alleviating the need for newcomers to acquire the knowledge and skills required to properly clean raw MRI data. 
