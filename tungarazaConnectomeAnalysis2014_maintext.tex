\section*{Introduction}

With its new emphasis on collecting larger datasets, data sharing, deep phenotying, and multimodal integration, neuroimaging has become a data intensive science. This is particularly true for connectomics where grass-root initiatives (e.g.the 1000 Functional Connectomes Project (FCP) \cite{Biswal2010}, the International Neuroimaging Data-sharing Initiative (INDI) \cite{Mennes2013}) and large-scale interational projects (the Human Connectome Project\cite{Sotiropoulus2013,VanEssen2012}, the Brain Connectome project from China\cite{Jiang2013}, the human brain project in EU known as CONNECT\cite{Assaf2013}) are collecting and openly sharing thousands of brain imaging datasets, each of which consist of hundreds of thousands of variables. Although this deluge of complex data promises to enable the investigation of neuroscientific questions that were previously inaccessible, it is quickly overwhelming the capacity of existing tools and algorithms to extract meaningful information from the data. This combined with a new focus on discovery science is creating a plethora of opportunities for data scientists from a wide range of disciplines such as computer science, engineering, mathematics, statistics, etc., to make substantial contributions to neuroscience. The goal of this review is to describe the state-of-the-art in connectomics research and enumerate opportunities for data scientists to contribute to the field.

The human connectome is a comprehesive map of the brain's circuitry, which consists of brain areas, their structural connections and their functional interactions. The connectome can be measured with a variety of different imaging techniques, but magnetic resonance imaging (MRI) is the most common primarily due to its near-ubiquity, non-invasiveness, and high spatial resolution. As measured by MRI: brain areas are patches of cortex (approximately $1 cm^3$ volume) containing $1,000$s of neurons. Structural connections are long range fiber tracts that are inferred from the motion of water particles measured by diffusion weighted MRI (dMRI), and functional connectivity is inferred from synchronized brain activity measured by functional MRI (fMRI). Addressing the current state-of-the-art for both functional and structural connectivity is well beyond the scope of a single review. Instead, this review will focus on functional connectivity, which is particularly fast growing and offers many exciting opportunities for data scientists.

The advent of functional connectivity analyses has popularized the application of discovery science to brain function, which marks a shift in emphasis from hypothesis testing, to supervised and unsupervised methods for learning statistical relationships from the data. Since functional connectivity is inferred from statistical dependencies between phsyiological measures of brain activity (i.e. correlations between the dependent variables), it can be measured without an experimental manipulation. Thus, functional connectivity is most commonly measured from "resting state" fMRI scans, during which the study participant is lying quietly and not performing an experimentor-specified task - when measured in this way, it is referred to as intrinsic functional connectivity (iFC). Once iFC is measured, data mining techniques can be applied to identify iFC patterns that covary with phenotypes, such as, indices of cognitive abilities, personality traits, or disease state, severity, and prognosis, to name a few. In a time dominated by skeptism about the ecological validity of psychiatric diagnoses, iFC analyses have become particularly important for identifying subgroups within patient populations by similarity in brain architecture, rather than similarity in symptom profiles. This new emphasis in discovery necessitates a new breed of data analysis tools that are equipped to deal with the issues inherent to functional neuroimaging data.

No matter what type of method is being applied, iFC analyses are plagued by the curse of dimensionality and an inability to validate their results with a gold standard. Functional MRI datasets typically involve hundreds or thousands of scans, each of which consist of a time series of hundreds of 3D brain volumes, that each contain measures from hundreds of thousands of brain locations (variables). Whether performing an analysis within a scan, or across scans, the number of variables $(N)$ is much larger than the number of observations $(P)$. Different analysis have approached the dimensionality problems in different ways, as described in the next sections, but there is no consensus on the best algorithm for brain data. Indeed there isn't a consensus about the best methods to use for any step of iFC analyses, and this lack of agreement is due to a lack of a ground truth or gold standard for comparing different techniques. Although several simulations have been proposed, there is always some skepticism about their accuracy and comprehensiveness. In their place, cross validation techniques have been utilized to compare methods based on some measure of reproducibility and/or generalizability.

This review is a primer on intrinsic functional connectivity analyses for interdisciplinary data scientists. It begins in section 2 by providing a description of fMRI data and the technical issues encountered when working with the data. Section 3 highlights several cutting edge iFC analytical paradigms that are being employed, and open issues that will benefit from an increased engagement of data science practitioners. The review concludes in section 4 with a list of several valuable open science resources that make connectomics research accessible to the larger scientific community.

\section{The Data}

Functional MRI, in its most conventional form, is a brain imaging modality in which approximately 40 3-mm, $64 \times 64$ voxel (3D pixel, $3\times3 mm^2$ in-plane resolution), brain slices are sequentially acquired every 2 - 3 seconds. The fMRI signal relies on the blood oxygenation level dependent (BOLD) contrast to derive a relative measure of brain function. The BOLD signal originates in the magnetic properties of hemoglobin, the protein in blood that binds to oxygen. When hemoglobin is not bound to oxygen (deoxygenated), its four atoms of iron generate a local magnetic field gradient that dephases a region of the fMRI signal. When bound to oxygen (oxygenated) the influence of the iron on the magnetic field is blocked, and the fMRI signal is preserved. The magnitude of the fMRI signal at image voxel is therefore proportional to the ratio between oxygenated and deoxygenated hemoglobin in or near that voxel. Perhaps quixotically, when brain activity increases, so does the oxy-deoxy ratio, resulting in an increase in the magnitude of the fMRI signal. 

The spatial and temporal properties of the fMRI signal are determined by properties of BOLD. The changes in BOLD due to brain activity (hemodynamic response), as measured using task-fMRI, are fairly slow, peaking 1-2 seconds after neuronal activity begins, and persisting 4-6 seconds after it ends. Resting state fluctuations that underpin iFC are consistently localized between $0.01-0.08$ Hz, with a peak somewhere around $0.035$ Hz. As a result, there is temporal auto-correlation in fMRI data. Since the BOLD signal originates from magnetic fields generated by blood, it tends to be localized to the larger vascular and smears out beyond an active brain region, resulting in spatial auto-correlation. As a consequence, care must be taken when modeling fMRI data to account for these two sources of autocorrelation (e.g. when estimating statistical significance).

There are several sources of noise for fMRI data including the scanning hardware, the protocol used to acquire the data, and the scanning participant. Noise due to the hardware tends to be minimal with modern equipment, as long as it is not malfunctioning. Thermal noise in the system electronics contribute Rician noise to the image and time series \cite{Gudbjartsson1995}. Changes in the operational characteristics of the imaging gradients can result in drifts in the fMRI signal over time \cite{Smith1999}. Data acquired in some older equipment would be plagued by psuedo-random spikes in the fMRI signal, but this tends to be rare in modern equipment that is working properly. As detailed in \cite{Craddock2013}, the type of imaging acquisition used and its parameters have a large impact on data quality. The misspecification of parameters can reduce the sensitivity of the signal to BOLD or markedly exacerbate the influences of other noise sources. Another consequence of the acquisition scheme are slice-to-slice variations in acquisition time that result in phase shifts in the fMRI signal that vary spatially. The physiology, behavior and anatomy of the participant being scanned also influences the fMRI signal. Heart beat generates a pulsing motion in the brain and consequently fluctuations in the fMRI signal. Changes in the thoracic volume of air due to breathing changes the magnetic field profile, which will result in instantaneous global shifts in image intensity. Also changes in rate and depth of breathing will lead to changes in the brain's oxygenation level and longer-term modulations of the fMRI signal. Perhaps worse of all is participant head-motion, which not only results in the misalignment of brain regions between brain volumes, but also modulates fMRI signal intensity due to partial volume and spin-history effects. 

\section*{Data preprocessing}

Several preprocessing procedures have been developed to deal with the various sources of noise described previously, but there is no consensus on which methods to use, and preprocessing is one of the most controversial areas in iFC analyses \cite{}. Voxel time courses can be temporally interpolated to the same time grid. Spatial coregistration between volumes can correct misalignment due to head motion. ALthough low frequency scanner drift can be filtered from the data, the slow sampling rate of typical fMRI (0.33 - 0.5 Hz) makes it impossible to filter out the higher frequency signal variations due to heart beat, respiration, and head motion. The most common approach for removing these noise sources, is to model their influence on the fMRI signal using a regression framework, and to subtract the result from the fMRI signal. For heart beat and respiration this can be accomplished using physiological recordings of these signals, but due to the difficulties of making these recordings in the scanning environment, surrogate signals from cerebrospinal fluid and white matter, which contain fluctuations due to physiological noise, but no neuronal signal are commonly used to model their effects. Several different methods have been proposed to account for head-motion induced fluctuations in the fMRI signal, and most employ a 6-, 12-, or 24-parameter model from the three translations and three rotations calculated during the coregistration procedure, with some evidence supporting the 24-parameter model as best. More aggressive methods censor offending time points by either deleting them all-together or in a regression model (spike regression). Some researchers have advocated regressing out the global-signal as a non-specific measure of noise, but this method is widely criticized since it mathematically centers the distribution of iFC values, which jeopardizes the interpretation of negative correlations \cite{}.  

Preprocessing fMRI data is reliant on the analysis of high resolution structural MRI data that is typically acquired during the same scanning session. It is from the structural data that we can segment the images into cerebral spinal fluid, white matter, and gray matter areas of the brain. Additionally the structural data is used to calculate a spatial transformation to a brain template to account for volumetric and morphometric differences between participants. In general, the quality of iFC analyses depends on the quality of the structural data and its processing.

\subsection*{IFC and Regional measures}

A variety of different strategies have been developed for analyzing iFC, that are outside what might be considered \emph{connectomes}-style analyses, but nevertheless are valuable methods for investigating the brain's functional architecture. iFC began with seed-based correlation analyses, which identify an iFC-map for a particular seed region from the Pearson's correlation of that regions mean timecourse with the timecourses of every other voxel in the brain \cite{Biswal2010}. This hypothesis-oriented approach has been complemented by unsupervised approaches such as clustering and independent components analysis for defining \emph{intrinsic connectivity networks} (ICN). Indeed using ICA researchers have identified 6-10 ICNs which are reproducible across participants, time, and across cognitive tasks. Several regional measures have been proposed that provide sensitivity to different aspects of the data's structure. The fractional amplitude of low frequency fluctuations is a normalized measure of each voxel's power in the frequency band commonly associated with iFC (0.01-0.08 Hz) and provides a relative measure of brain activity. Regional homogeneity (ReHo) measures the degree to which a voxel's 6-, 18-, or 22- voxel neighborhood are synchronized. Voxel-mirrored homotopic connectivity is measured from the correlation between a voxel's timecourse and the timecourse for its corresponding voxel in the contralateral hemisphere. Another approach, which is measured from the time-lag that maximizes correlation between a voxel's timecourse and a reference timecourse such as the global signal, has been shown to be sensitive to blood flow deficits, particularly in stroke \cite{lv}. The result of each of these methods is a voxel-wise map of statistics that can be compared across participants using either voxel-by-voxel univarite statistical tests such as t-tests or anovas, or can be analyzed using multivariate supervised learning techniques.

\section{The Connectome Analysis Paradigm}

In 2005 Sporn and Hagmann \cite{Sporns2005,Hagmann2005} independently and in parallel coined the term \textit{the human connectome}, which embodies the notion that the set of all connections within the human brain can be represented and understood as graphs. In the context of iFC: graphs provide a mathematical representation of the functional interactions between brain areas. Any graph analysis problem follows the following procedural steps: (1) identification of nodes, (2) identification of edges i.e. the associations between nodes, (3) analysis of the graph i.e. the structure and connections of the resulting graph in order to identify any expected and novel patterns, and (4) validation of the resulting patterns. Analysis of the human connectome adheres to these steps as outlined in this section.

In a graph, brain areas are represented as \emph{nodes} and the similarity between the brain activity of two nodes is represented by an \emph{edge} between them. Edges in iFC graphs are typically undirected, since direction of influence between nodes is unknown, and carries a weight to indicate the degree of similarity between the nodes. 
 
\subsection*{Defining the nodes of the connnectome}

To form an accurate representation of the human connectome using graph theoretic approaches, we need to define nodes. For fMRI data an obvious choice might be to use individual voxels as nodes, but that approach will result in a computationally expensive graphs that can contain billions of connections. Fortunately, due to the spatial correlation inherent in fMRI data the timecourses of neighboring voxels are very similar, permitting them to be combined together into larger homogenous brain areas. It is important that nodes in the connectome contain homogenous functional information, as a previous simulation study has shown that mixing timecourses between nodes will significantly impair the ability to accurately reproduce the underlying graph structure \cite{smith2009}. A few different strategies remain for determining brain areas, including anatomical atlases \cite{}, meta-analyses of the literature \cite{}, and unsupervised or semi-supervised learning approaches \cite{}. Anatomical atlases subdivide the brain based on anatomical landmarks or into areas containing similar cell types, as determined by microscope analysis of post-mortem brains, but since their definition did not incorporate functional information, and they are not likely to be functionally homogeneous. Meta-analytic approaches define regions from the overlap of activations found in task-based nueroimaging studies, or from coordinates commonly reported in the iFC literature. When using this method, care should be taken to insure that brain regions are similar in size, and are non-overlapping. In the last approach, a clustering algorithm is used to sub-divide the funcitonal data into homogenous units. Several different approaches for the data-driven node specification have been defined, that vary in aspects such as definition of clustering cost, incorporation of explicit constraints, and whether they are performed at the individual or group-level.  This prompts the inevitable question of which parcellation is the best representation of the human connectome. Overall there is no concensus on which is the best node definition to use, although their appears to be concensus that data-driven approaches outperform anatomical atlases (figure from cameron paper).

A considerable drawback of data-driven approaches for defining brain regions, is that the number of regions in the clustering solution, or conversely their size, must be specified. The choice of node size has been shown to have a considerable effect on the resulting graph topology \cite{Zalesky}. It can also affect the outcome of a particular study. For instance, in trying to identify the brain areas that cause disruption in brain function in Schizophrenic individuals, Cecci et. al. \cite{Cecci_2009} found that when they used individual voxels to create the brain graph they were able to identify statistically significant differences in the brain networks of Schizophrenic individuals compared to normal controls. This difference however, disappeared when they used larger brain areas. There are a variety of mechanisms for optimizing the number of brain areas in a parcellation, many of which using cross-validation methods, but in application they do not tend to converge to a single optimum, and instead provide a range of parcellations that are suitable. The choice of parcellation used will strictly depend on the problem to be solved by that analysis and the amount of error that one can allow when interpreting the results. 

When comparing graphs between participants, it is necessary for the graphs to be aligned, which requires their to be a one-to-one correspondance between nodes across participants. This is typically satisfied for nueroimaging data by using a single node definition that is warped to each individuals brain, using a nonlinear transform that accounts for variation in brain size and morphometry. This comes at a loss of information since their will always be a mismatch between the group node specification and the individual brain due to coregistration error, and idiosyncracies in brain structure. Unsurprisingly, individual-level data-driven parcellations have been shown to perform better the participant that they are generated from, then group-level parcellations. But  comparing graphs constructed with participant-specific node specifications will require either finding an alignment between the resulting graphs or by comparing statistics calculated from the graph that are invariant to node specification. The former is a daunting task and the latter abstracts away alot of the information available in the graph structure. But graphs with different node definitions will likely be a certainty when studying populations with widely different brain morphometry, such as in development, aging, and neurological disorders. \textit{Add in lfcd}

\subsection*{Define the edges}
After deciding the grain size of each region, i.e. whether it be an individual voxel or a set of spatially contigous voxels or even a set of ovelapping regions (one voxel can belong to more than one node of the graph e.g. ICA approach), one needs to construct the edges that describe relationships between nodes. While general graphs can have multiple edges between two nodes, brain graphs tend to be simple graphs with a single undirected edge between pairs of nodes (ie the direction of influence between nodes is unknown). Additionally edges in graphs of brain function tend to be weighted or annotated with a value that indicates the similarity between nodes. 

The first step in defining edges is summarizing the functional information present in the node. A representative time course for each node may be created by either average the timecourses of all the voxels in a node, or by taking the first eigenvariate from a single value decomposition of all of the time courses. The former is preferred because it has a tendancy to represent all of the time courses in a node equally, rather than favoring those that are most commonly occuring \cite{craddock2012}. These summary time courses can either be compared directly between nodes, or can be used in a seed-based correlation analysis to construct node-specific whole brain iFC maps, which are then compared spatially between nodes. Although, these two features are seemingly similar, they can lead to very different similarities between nodes \cite{craddock2012}. 

Quite a few different measures have been used in the iFC literature for measuring the similarity between time courses, many of which have been reviewed in an extensive simulation study \cite{smith2010}. By far the most commonly employed measure is Pearon's correlation coefficient, which performs well in simulation. A benefit of Pearson's correlation is that is scaled so that it is insensitive to differences in scales and shifts between time courses. This is particularly useful for fMRI, which is a relative measure of brain funciton that can vary widely across the brain for reasons that are non-neuronal in source, i.e. proximity to a vein. A draw back is that Pearson's correlation is a bivariate measure that does not adequately address the simultaneous influence that mutliple nodes have on one another. The resulting loss of specificity might result in "phantom" edges between regions that is a by-product of shared variance from a mutual relationship with another region (or several other regions). Partial correlation methods simultaneously take into account all of the nodes in their calculation and as such have higher specificity. But, since their are often more nodes in a graph than independent observations (particularly when accounting for temporal autocorrelation) partial correlations cannot be directly measured from the data. Instead, regularized regression techniques can be employed. Approaches for measuring the spatial simlarity between iFC maps has been much less studied in the literature. Commonly employed measures include $\eta^2$ and the concordance correlation coefficient, both of which measure how identical the two maps are, included differences in shifts and scales. We have yet to find a report where partial correlation was used to calculate similarity between iFC maps.

Once the similarity between nodes has been calculated, a threshold must be chosen to use to indicate the presence of an edge. Negative relationships are possible given the commonly employed similarity measures, but these are typically removed from the graph since it is unclear how to interpret negative similarity, and because many graph theoretical mathematical techniques require the weights to be nonnegative. It is common to apply a significance threshold to the graph, which is chosen to minimize the number of edges that arise by chance. These thresholds can be determined by parametric tests such as Pearson's correlation to p-value calculators, or non-parametric tests such as wave-strapping, circular block boostrap, or phase randomization. It is import to correctly account for the auto-correlation inherent in the data when employing these techniques. Alternatively, a sparsety threshold may be employed to choose a percentage of the edges with the strongest weights. This is useful when calculating graph statistics that are highly sensitive to the number of edges in the graph, and may serve to ameliorate batch-effects that may exist between datasets acquired on different scanners or at different sites. Once the below-threshold edges have been excluded from the brain graphs, the are ready for group-level analysis. 

\section*{Comparing brain graphs} 
Topology analysis (to identify both local and global features), predictive modeling, and clustering are three of the most popular analysis at the macroscale level that are done on the human connectome \cite{}. In each of these analysis, researchers seek to compare brain graph from different cohorts, or from the same subject but under different experimental conditions. In order to facilitate this, one needs to have some mechanism for comparing one connectome to another.  

\subsection*{How to measure similarity/distance between graphs}
Comparing connectomes across subjects involves comparing graphs with the same cardinality and each node semantically representing the same entity. In general graphs, the first task during graph comparison is to solve the node assignment problem between graphs. In brain graphs this problem does not exist because of the way these graphs are modeled. 

Approaches to comparing brain graphs can be divided into two types: those that involve all vertices of a graph and those that look at only a subset of the vertices of a graph (or sub-graph). For the former, the most common approach is to transform the graph from an adjacency matrix representation to a vector representation using different types of graph-to-vector mapping techniques \cite{Ravindran}. One such approach, referred to as \emph{bag of edges} is to treat each edge in the adjacency matrix as a sample from some random variable. Thus, a set of $N$ brain graphs each with $M$ edges will have $N$ datapoints for each of the $M$ random variables. Thereafter one can use any of the well explored similarity/dissimilarity metrics used in machine learning or data mining algorithms. One of the benefits of this representation is the ability to perform mass univariate analysis on these graphs especially if they come from different cohorts, or are from the same subject but under different experimental conditions \cite{Hutchinsons2013, Ginestet2013}. Researchers have used methods such as Network Based Statistic \cite{Zalesky2011}, Spatial Pairwise Clustering \cite{Zalesky2012}, Statistical Parametric Networks \cite{Ginestet2013} to perform hypothesis testing on brain graphs after mapping them into such vector space. The vector representation of graphs has also been succeffully used in multivariate studies as described in \cite{Milham2012, Shehzad2013} via their connectome wide association studies, which is a way to "attribute phenotypic variation among individuals to differences in the macro and microarchitecture of the human connectome". These approaches have been especially successful because they manage to evade the multiple comparison problem. Other researchers \cite{Richiardi2011, Craddock2009} have used  used the vector representation  of brain graphs together with some feature selection efforts to perform classification on these graphs. Despite these success stories, the drawbacks of representing a brain graph as a vector is that this representation throws away all information about the structure of the graph.

Another approach for graph similarity using all the vertexes involves computing a set of \emph{graph-invariants} such as node centrality, modality, global efficiency e.t.c. and using the values of these measures to represent the graph \cite{rubinov} \textit{find more}. The advantages of this approach is that they do not require graphs to be aligned, and can substantially reduce the dimensionality of the graph comparison problem. On the other hand, representing the graph using its computed invariants throws away information about that graph's vertex labels \cite{Vogelstein2012}. Moreover, after computing these invariants it is often unclear how they can be interpreted biologically.

 In an effort to overcome these limitations, work is being done to look at sub-graphs \cite{}   

\textit{how about directly comparing graphs using a graph similarity metric}


\section*{Future Trends}

\subsection*{Dynamic Connectivity}
One important assumption that is made when building brain networks is that the fMRI signal is stationary. This simplification affords the use of correlation coefficients (computed using the entire representative time-course of each brain region) for representing the statistical dependence between brain regions. Recent studies \cite{Hutchinson2013, Fu2013,} however, have demonstrated that removing this assumption and using techniques that rightfully assume that the fMRI signal is not stationary across time, results in brain networks that show changes in connectivity patterns between brain regions over time. These results mirror what scientists have come to expect; brain regions change allegiances during the course of executing various tasks.

To date trying to model the human connectome via taking into account its dynamic nature posses a number of unsolved problems \cite{Hutchinson2013} some of which include: (1) identifying and removing contributions to the modeled functional connectome due to subject motion, scanner noise, and physiological noise, (2) given that in dynamic connectivity the "brain network" changes with time, what is the best way to define its nodes and edges (ie. quantify the dynamic connectivity), (3) what are the roles of individual brain regions (including subcortical structures and brain stem) in regulating these dynamic networks, and (4) are there specific instances of network configurations that are repeated/revisited over time? For more open problems and a detailed explanation of work done in the area of dynamic functional connectivity see \cite{Hutchinson2013}.

\subsection*{Causality}
To date, a lot of work has been done to identify nodes within a brain graph that share some degree of mutual information (known as functional connectivity). Another interesting question to answer with regards to brain graphs is to identify what nodes have direct influence on the signal obtained in other nodes (known as effective connectivity). In other words, how can we identify the directional causal influence of one brain region over another using brain graphs? This is a hard problem to tackle using rsfMRI. Simple correlation analysis does not suffice because a "high correlation between remote sampling sites might imply some direct connection, or it might imply some third site driving their joint activation, or it might imply them jointly driving some third site. And even if they are connected, it's difficult to tell the direction of the connection, even if there is a "direction" to it." \footnote{http://mindhive.mit.edu/node/58}. 

Current approaches include the use of dynamic causal modeling (DCM) \cite{}, structural equation modeling (SEM) \cite{}, and Granger Causality \cite{}. While these methods have shed some light into the wiring of the brain, there are some unresolved problems. DCM analysis requires some external stimuli that will perturb a given brain networks. Such a stimuli is not possible for rsfMRI data because it is acquired when the subject is at rest. On the other hand, using the GC approach one attempts to identify a specific node that influences the characteristic of another node in a given brain network. The drawback of this approach is that the hemodynamic response in fMRI signal naturally lags by a few seconds. Thus, it is non-trivial to draw conclusions that any observed lag in signal is specifically due to one node exerting an influence over another.   

\subsection*{Prediction}
There is an increase in the use of machine learning techniques especially supervised learning in neuroimaging for predicting various aspects of brain function and/or disfunction. Researchers have been able to train computers to learn patterns that can discriminate between different cohorts e.g. disease individuals versus healthy controls \cite{Richiardi2013}. The learnt patterns are then used to identify the distinguishing characteristics between the cohorts. For some learning algorithms, the resulting characteristics could subsequently be used to validate some hypothesis about brain function and/or malfunction \cite{} e.g. identifying patterns that result into neurodegenerative disorders \cite{WeeMICCAI2013}. 

The three basic steps in supervised learning are: (1) generate the features (independent variables), (2) give each subject a label representing what group they belong to (dependent variable), and (3) apply an appropriate machine learning algorithm to the generated features and labels. The features for the brain graphs can be (1) a set of topological properties from each brain graph \cite{Cecci2009, Bassett2012}, (2) a vector embedding of the brain graphs \cite{Richiadi2013,Luo2003}, or (3) the result of passing the brain graphs through a graph kernel \cite{}. 

As with other predictive problems in data analysis that use graphs as features, there are several open problems that need to be investigated: (1) Feature Representation: what is the appropriate approach for presenting each brain graph to the learning algorithms. Is it one of the above mentioned approaches or could one create new representations that will make the learning algorithms more sensitive to the differences and similarities between brain graphs. (2) Scalability: as the imaging technology is getting better over time, we expect the resolution of fMRI images to also improve enabling the generation of brain graph at finer scales (ie. with more regions). How can we best scale available learning algorithms to accommodate this change? As noted by Richiardi et. al. \cite{Richiardi} a typical learning algorithm to date can only handle graphs with up to a hundred or a few thousand nodes at best. And even then, the available data tends to have much less observations compared to the predictors (brain voxels/regions). This has been shown to cause poor performance of many learning algorithms \cite{Hastie}. A potential solution is to use regularization techniques \cite{WeeMICCAI}. Are there other approaches that could be used? (3) How can we account for the bias introduced during the process of estimating statistical dependence between regions? This bias comes from the number of timepoints available for each time-course (the larger this value the more confident we can be that the resulting correlation coefficient is trully different from zero), significant variations in size of many brain regions, physiological noise, and noise with spatial characteristics \cite{Richiardi}

\section*{Conclusion}

\section*{Open Science Resources}
As with other big data problems, the analysis of the type of brain data described above will benefit significantly from open science and open data. By open science we mean the process of making intermediate and end data products available to researchers outside the lab that initiated the investigation and by open data we mean data sharing, data reuse, allowing groups other than the ones that acquired the data to further analyze freely available data \cite{Milham2012}. Both open science and open data have the benefit of facilitating the creation of new questions to answer from a data set that the people who acquired the data didn't conceive. A great example of this is the bioinformatics community, which came into existence due to the availability of large genomics datasets that were publicly available and could be used by anybody who possessed the skills and knowledge to mine them\cite{VanHorn2013}. This brought scientists and researchers from disparate fields (ranging from biology, statistics, to computer science and engineering) together. The same can be aspired for the brain imaging community by making data acquired in different labs publicly available. At the moment there are a few public databases such as the fMRI data center \footnote{fmri datacenter webaddress}, LONI IDA \footnote{LONI webaddress}, 1000 Functional Connectomes Project (FCP)\footnote{\url{http://fcon_1000.projects.nitrc.org}}, the International Neuroimaging Data-sharing Initiative (INDI)\footnote{INDI webaddress}, ABIDE preprocessed\footnote{\url{http://preprocessed-connectomes-project.github.io/abide/}}, Human Connectome Project \footnote{HCP webaddress}, ADHD 200 preprocessed \footnote{\url{http://neurobureau.projects.nitrc.org/ADHD200/Introduction.html}},CORR \footnote{\url{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/index.html}}. Among these, there are a few such as the data from the Human Connectome Project and the Cameron data set, which contain clean and tidy data. The latter will speed up knowledge discovery and new method development by alleviating the need for newcomers to acquire the knowledge and skills required to properly clean raw MRI data. 
